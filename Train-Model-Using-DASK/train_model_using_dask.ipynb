{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Traning diabetes model from parquet files using DASK dataframe\r\n",
        "\r\n",
        "In this tutorial we explore DASK dataframe and its Api to train a diabetes model.\r\n",
        "\r\n",
        "This tutorial includes the following:\r\n",
        "- Convert a csv file into parquet files, partition on a column.\r\n",
        "- We train a model from the parquet files."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install DASK dataframe using pip"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dask"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201408703
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a folder to store the experiment files "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "# Create a folder for the experiment files\r\n",
        "training_folder = 'diabetes-training'\r\n",
        "os.makedirs(training_folder, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201433929
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy the csv file in the traning folder and convert into parquet files\r\n",
        "\r\n",
        "Note: We can skip the copy part and directly convert the file into parquet files. Provide the correct path for the files."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert a csv file into a parquet file\r\n",
        "\r\n",
        "import dask.dataframe as dd\r\n",
        "import os, shutil\r\n",
        " \r\n",
        "# Copy the data file into the experiment folder\r\n",
        "shutil.copy('diabetes.csv', os.path.join(training_folder, \"diabetes.csv\"))\r\n",
        "csv_path = os.path.join(os.getcwd(), training_folder)\r\n",
        "\r\n",
        "# Read the csv file in dask data frame and convert into parquet files  \r\n",
        "diabetes_ddf =  dd.read_csv(csv_path+\"/diabetes.csv\")\r\n",
        "diabetes_ddf.to_parquet(csv_path, write_index=False, partition_on='Diabetic')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201440637
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create workspace\r\n",
        "If the workspace already exists connect to it"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.create(\r\n",
        "    name = \"Your Workspace Name\",\r\n",
        "    subscription_id = \"Your Subsription Id\",\r\n",
        "    resource_group = \"Your Resource Group\", \r\n",
        "    location = \"Your location\",  # e.g \"westus\"\r\n",
        "    exist_ok = True,\r\n",
        "    show_output = True)\r\n",
        "\r\n",
        "ws.write_config()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "\r\n",
        "# Load the workspace from the saved config file\r\n",
        "ws = Workspace.from_config()\r\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201452631
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the taining folder in the default datastore "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the default datastore\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Enumerate all datastores, indicating which is the default\r\n",
        "for ds_name in ws.datastores:\r\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201454639
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading file to workspace blob store\r\n",
        "\r\n",
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "Dataset.File.upload_directory(src_dir='diabetes-training',\r\n",
        "                              target=DataPath(default_ds, 'dask-diabetes-training/')\r\n",
        "                              )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201463053
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a parquet files dataset & register in the workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "#Create a file dataset from the path on the datastore (this may take a short while)\r\n",
        "file_data_set = Dataset.File.from_files(path=(default_ds, 'dask-diabetes-training/*/*.parquet'))\r\n",
        "\r\n",
        "# Get the files in the dataset\r\n",
        "for file_path in file_data_set.to_path():\r\n",
        "    print(file_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201718722
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the file dataset\r\n",
        "try:\r\n",
        "    file_data_set = file_data_set.register(workspace=ws,\r\n",
        "                                            name='parquet diabetes file dataset',\r\n",
        "                                            description='parquet files',\r\n",
        "                                            tags = {'format':'parquet'},\r\n",
        "                                            create_new_version=True)\r\n",
        "except Exception as ex:\r\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201730719
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print all workspce datasets with the following command"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Datasets:\")\r\n",
        "for dataset_name in list(ws.datasets.keys()):\r\n",
        "    dataset = Dataset.get_by_name(ws, dataset_name)\r\n",
        "    print(\"\\t\", dataset.name, 'version', dataset.version)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201734907
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a model from a parquet files dataset\r\n",
        "- A script that trains a classification model by using a parquet files dataset that is passed to is as an input."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $training_folder/diabetes_parquet_training.py\r\n",
        "# Import libraries\r\n",
        "from azureml.core import Run\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "import os\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "import dask.dataframe as dd\r\n",
        "import argparse\r\n",
        "\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument('--input-data', type=str, dest='dataset_folder', help='data mount point')\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "run = Run.get_context()\r\n",
        "# load the diabetes dataset\r\n",
        "print(\"Loading Data...\")\r\n",
        "\r\n",
        "data_path = args.dataset_folder\r\n",
        "diabetes = dd.read_parquet(path=[data_path+'/Diabetic=*/*.parquet'], engine='pyarrow')\r\n",
        "\r\n",
        "# Separate features and labels\r\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].map_partitions(np.asarray)\r\n",
        "\r\n",
        "# Split data into training set and test set\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.compute(), y.compute(), test_size=0.30, random_state=0)\r\n",
        "\r\n",
        "# Set regularization hyperparameter\r\n",
        "reg = 0.01\r\n",
        "\r\n",
        "# Train a logistic regression model\r\n",
        "print('Training a logistic regression model with regularization rate of', reg)\r\n",
        "run.log('Regularization Rate',  np.float(reg))\r\n",
        "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\r\n",
        "\r\n",
        "# calculate accuracy\r\n",
        "y_hat = model.predict(X_test)\r\n",
        "acc = np.average(y_hat == y_test)\r\n",
        "print('Accuracy:', acc)\r\n",
        "run.log('Accuracy', np.float(acc))\r\n",
        "\r\n",
        "# calculate AUC\r\n",
        "y_scores = model.predict_proba(X_test)\r\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\r\n",
        "print('AUC: ' + str(auc))\r\n",
        "run.log('AUC', np.float(auc))\r\n",
        "\r\n",
        "# Save the trained model in the outputs folder\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "joblib.dump(value=model, filename='outputs/diabetes_parquet_model.pkl')\r\n",
        "\r\n",
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1667377490731
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the training script as an experiment\r\n",
        "The conda environment is built on-demand the first time the experiment is run, and cached for future runs that use the same configuration; so the first run will take a little longer."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "# Create a Python environment for the experiment (from a .yml file)\r\n",
        "env = Environment.from_conda_specification(\"dask_frame_env\", \"environment.yml\")\r\n",
        "diabetes_parquet_ds = ws.datasets.get(\"parquet diabetes file dataset\")\r\n",
        "# Create a script config\r\n",
        "script_config = ScriptRunConfig(source_directory=training_folder,\r\n",
        "                                script='diabetes_parquet_training.py',\r\n",
        "                                arguments = ['--input-data', diabetes_parquet_ds.as_download(path_on_compute=\"/tmp/training_files\")], # Reference to dataset location,\r\n",
        "                                environment=env,\r\n",
        "                                ) \r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201755981
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the files, if they are already exist \r\n",
        "import shutil\r\n",
        "download_file_path = \"/tmp/training_files\"\r\n",
        "if os.path.exists(download_file_path):\r\n",
        "    shutil.rmtree(download_file_path)\r\n",
        "\r\n",
        "# submit the experiment run\r\n",
        "experiment_name = 'parquet-train-diabetes'\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
        "run = experiment.submit(config=script_config)\r\n",
        "\r\n",
        "# Show the running experiment run in the notebook widget\r\n",
        "RunDetails(run).show()\r\n",
        "\r\n",
        "# Block until the experiment run has completed\r\n",
        "run.wait_for_completion()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201777975
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the trained model\r\n",
        "Note that the outputs of the experiment include the trained model file (**diabetes_parquet_model.pkl**). We can register this model in your AML workspace, making it possible to track model versions and retrieve them later."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the model\r\n",
        "from azureml.core import Model\r\n",
        "\r\n",
        "run.register_model(model_path='outputs/diabetes_parquet_model.pkl', model_name='diabetes_parquet_model',\r\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\r\n",
        "\r\n",
        "# List registered models\r\n",
        "for model in Model.list(ws):\r\n",
        "    print(model.name, 'version:', model.version)\r\n",
        "    for tag_name in model.tags:\r\n",
        "        tag = model.tags[tag_name]\r\n",
        "        print ('\\t',tag_name, ':', tag)\r\n",
        "    for prop_name in model.properties:\r\n",
        "        prop = model.properties[prop_name]\r\n",
        "        print ('\\t',prop_name, ':', prop)\r\n",
        "    print('\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669201837106
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}