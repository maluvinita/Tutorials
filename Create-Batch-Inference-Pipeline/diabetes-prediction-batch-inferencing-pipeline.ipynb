{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Train a diabetes prediction model and create a batch inferencing pipeline. \r\n",
        "[This dataset](https://github.com/maluvinita/Tutorials/tree/main/data) is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\r\n",
        "\r\n",
        "This tutorial shows how we can use a trained model for prediction, if we have stored our data in different files.  \r\n",
        "\r\n",
        "This tutorial includes following:\r\n",
        "- Train the model using diabetes dataset & register it.\r\n",
        "- Create a batch inferencing pipeline. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing AML packages"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\r\n",
        "import os\r\n",
        "import azureml.core\r\n",
        "from azureml.core import Workspace\r\n",
        "from azureml.core import Experiment\r\n",
        "from azureml.core.compute import AmlCompute\r\n",
        "from azureml.core.compute import ComputeTarget\r\n",
        "from azureml.core.environment import Environment\r\n",
        "from azureml.core.conda_dependencies import CondaDependencies\r\n",
        "\r\n",
        "print(sys.version)\r\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669191977084
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create workspace\r\n",
        "If the workspace already exists connect to it"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.create(\r\n",
        "    name = \"Your Workspace Name\",\r\n",
        "    subscription_id = \"Your Subsription Id\",\r\n",
        "    resource_group = \"Your Resource Group\", \r\n",
        "    location = \"Your location\",  # e.g \"westus\"\r\n",
        "    exist_ok = True,\r\n",
        "    show_output = True)\r\n",
        "\r\n",
        "ws.write_config()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192500331
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\r\n",
        "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669191988395
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_We already learned how to register a data set in our workspace in [this tutorial ](https://github.com/maluvinita/Tutorials/blob/main/Deploy-Real-Time-Inferencing-Service/diabetes-prediction-model-AML.ipynb). Therfore we directly use that registered dataset in this tutorial to train the model. _"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a model from a file dataset\r\n",
        "Run the following two code cells to create:\r\n",
        "\r\n",
        "- A folder named diabetes_training\r\n",
        "- A script that trains a classification model by using a file dataset that is passed to is as an input."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "\r\n",
        "# Create a folder for the experiment files\r\n",
        "experiment_folder = 'diabetes_training'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "print(experiment_folder, 'folder created')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192019834
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/diabetes_training.py\r\n",
        "# Import libraries\r\n",
        "import os\r\n",
        "import argparse\r\n",
        "from azureml.core import Dataset, Run\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import joblib\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "import glob\r\n",
        "\r\n",
        "# Get script arguments (rgularization rate and file dataset mount point)\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\r\n",
        "parser.add_argument('--input-data', type=str, dest='dataset_folder', help='data mount point')\r\n",
        "args = parser.parse_args()\r\n",
        "\r\n",
        "# Set regularization hyperparameter (passed as an argument to the script)\r\n",
        "reg = args.reg_rate\r\n",
        "\r\n",
        "# Get the experiment run context\r\n",
        "run = Run.get_context()\r\n",
        "\r\n",
        "# load the diabetes dataset\r\n",
        "print(\"Loading Data...\")\r\n",
        "data_path = args.dataset_folder # Get the training data path from the input\r\n",
        "# (You could also just use args.dataset_folder if you don't want to rely on a hard-coded friendly name)\r\n",
        "\r\n",
        "# Read the files\r\n",
        "all_files = glob.glob(data_path + \"/*.csv\")\r\n",
        "diabetes = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\r\n",
        "\r\n",
        "# Separate features and labels\r\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\r\n",
        "\r\n",
        "# Split data into training set and test set\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\r\n",
        "\r\n",
        "# Train a logistic regression model\r\n",
        "print('Training a logistic regression model with regularization rate of', reg)\r\n",
        "run.log('Regularization Rate',  np.float(reg))\r\n",
        "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\r\n",
        "\r\n",
        "# calculate accuracy\r\n",
        "y_hat = model.predict(X_test)\r\n",
        "acc = np.average(y_hat == y_test)\r\n",
        "print('Accuracy:', acc)\r\n",
        "run.log('Accuracy', np.float(acc))\r\n",
        "\r\n",
        "# calculate AUC\r\n",
        "y_scores = model.predict_proba(X_test)\r\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\r\n",
        "print('AUC: ' + str(auc))\r\n",
        "run.log('AUC', np.float(auc))\r\n",
        "\r\n",
        "model_file = 'diabetes_pipeline_model.pkl'\r\n",
        "os.makedirs('outputs', exist_ok=True)\r\n",
        "# note file saved in the outputs folder is automatically uploaded into experiment record\r\n",
        "joblib.dump(value=model, filename='outputs/'+model_file)\r\n",
        "\r\n",
        "run.complete()\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the training script as an experiment\r\n",
        "The conda environment is built on-demand the first time the experiment is run, and cached for future runs that use the same configuration; so the first run will take a little longer."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
        "from azureml.core.runconfig import DockerConfiguration\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "import shutil\r\n",
        "\r\n",
        "# Get the training dataset\r\n",
        "diabetes_ds = ws.datasets.get(\"diabetes file\")\r\n",
        "\r\n",
        "# Create a Python environment for the experiment (from a .yml file)\r\n",
        "env = Environment.from_conda_specification(\"pipeline_exp_environment\", \"environment.yml\")\r\n",
        "\r\n",
        "# Create a script config\r\n",
        "script_config = ScriptRunConfig(source_directory=experiment_folder,\r\n",
        "                                script='diabetes_training.py',\r\n",
        "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\r\n",
        "                                             '--input-data', diabetes_ds.as_download(path_on_compute=\"/tmp/training_files\")], # Reference to dataset location\r\n",
        "                                environment=env, # Use the environment created previously\r\n",
        "                                )\r\n",
        "\r\n",
        "\r\n",
        "#Create an experiment to track the runs in your workspace\r\n",
        "experiment_name = 'predict-diabetes-pipeline'\r\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\r\n",
        "\r\n",
        "# remove the existing file becuse it doesn't overwrite the file \r\n",
        "import shutil\r\n",
        "download_file_path = \"/tmp/training_files\"\r\n",
        "if os.path.exists(download_file_path):\r\n",
        "    shutil.rmtree(download_file_path)\r\n",
        "\r\n",
        "# submit the experiment\r\n",
        "run = experiment.submit(config=script_config)\r\n",
        "RunDetails(run).show()\r\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192055906
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the trained model\r\n",
        "Note that the outputs of the experiment include the trained model file (**diabetes_pipeline_model.pkl**). We can register this model in your AML workspace, making it possible to track model versions and retrieve them later."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the model\r\n",
        "from azureml.core import Model\r\n",
        "\r\n",
        "run.register_model(model_path='outputs/diabetes_pipeline_model.pkl', model_name='diabetes_pipeline_model',\r\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\r\n",
        "\r\n",
        "# List registered models\r\n",
        "for model in Model.list(ws):\r\n",
        "    print(model.name, 'version:', model.version)\r\n",
        "    for tag_name in model.tags:\r\n",
        "        tag = model.tags[tag_name]\r\n",
        "        print ('\\t',tag_name, ':', tag)\r\n",
        "    for prop_name in model.properties:\r\n",
        "        prop = model.properties[prop_name]\r\n",
        "        print ('\\t',prop_name, ':', prop)\r\n",
        "    print('\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192065196
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model in current working directory.\r\n",
        "We need this model while we create a pipeline. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_obj = Model(ws, 'diabetes_pipeline_model')\r\n",
        "model_path = model_obj.download(exist_ok = True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192102386
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate batch data\r\n",
        "\r\n",
        "We will generate a random sample from our diabetes CSV file, upload that data to a datastore in the Azure Machine Learning workspace, and register a dataset for it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "import os\r\n",
        "\r\n",
        "# Set default data store\r\n",
        "ws.set_default_datastore('workspaceblobstore')\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "\r\n",
        "# Enumerate all datastores, indicating which is the default\r\n",
        "for ds_name in ws.datastores:\r\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\r\n",
        "\r\n",
        "# Load the diabetes data\r\n",
        "diabetes = pd.read_csv('diabetes-data/diabetes.csv')\r\n",
        "# Get a 100-item sample of the feature columns (not the diabetic label)\r\n",
        "sample = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].sample(n=100).values\r\n",
        "\r\n",
        "# Create a folder\r\n",
        "batch_folder = './batch-data'\r\n",
        "os.makedirs(batch_folder, exist_ok=True)\r\n",
        "print(\"Folder created!\")\r\n",
        "\r\n",
        "# Save each sample as a separate file\r\n",
        "print(\"Saving files...\")\r\n",
        "for i in range(100):\r\n",
        "    fname = str(i+1) + '.csv'\r\n",
        "    sample[i].tofile(os.path.join(batch_folder, fname), sep=\",\")\r\n",
        "print(\"files saved!\")\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669127311282
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload batch data\r\n",
        "Upload the data in the workspaceblob store.\r\n",
        "\r\n",
        "_Note : please remove all the hidden files eg .amlignore etc. if those were created during generation of batch data _   "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the files to the default datastore\r\n",
        "print(\"Uploading files to datastore...\")\r\n",
        "default_ds = ws.get_default_datastore()\r\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-dataset\", overwrite=True, show_progress=True)\r\n",
        "\r\n",
        "print(\"Done!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669136712716
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register a dataset for the batch input data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "\r\n",
        "# Register a dataset for the input data\r\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-dataset/'))\r\n",
        "\r\n",
        "try:\r\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \r\n",
        "                                             name='batch-dataset',\r\n",
        "                                             description='batch-data',\r\n",
        "                                             create_new_version=True)\r\n",
        "except Exception as ex:\r\n",
        "    print(ex)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192216422
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a pipeline for batch inferencing\r\n",
        "\r\n",
        "Now we're ready to define the pipeline we'll use for batch inferencing. Our pipeline will need Python code to perform the batch inferencing, so let's create a folder where we can keep all the files used by the pipeline"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "# Create a folder for the experiment files\r\n",
        "experiment_folder = 'batch_pipeline'\r\n",
        "os.makedirs(experiment_folder, exist_ok=True)\r\n",
        "\r\n",
        "print(experiment_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192233313
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll create a Python script to do the actual work, and save it in the pipeline folder:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/batch_diabetes.py\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from azureml.core import Model\r\n",
        "import joblib\r\n",
        "\r\n",
        "\r\n",
        "def init():\r\n",
        "    global model\r\n",
        "    # Get the path to the deployed model file and load it   \r\n",
        "    model_path = Model.get_model_path('diabetes_pipeline_model')\r\n",
        "    model = joblib.load(model_path)\r\n",
        "   \r\n",
        "\r\n",
        "def run(mini_batch):\r\n",
        "    # This runs for each batch\r\n",
        "    resultList = []\r\n",
        "    # process each file in the batch\r\n",
        "    for f in mini_batch:\r\n",
        "        # Read the comma-delimited data into an array\r\n",
        "        data = np.genfromtxt(f, delimiter=',')\r\n",
        "        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\r\n",
        "        prediction = model.predict(data.reshape(1, -1))\r\n",
        "        # Append prediction to results\r\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\r\n",
        "    return resultList"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create compute\r\n",
        "We'll need a compute context for the pipeline, so we'll use the following code to specify an Azure Machine Learning compute cluster (it will be created if it doesn't already exist)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "cluster_name = \"your cluster name\"\r\n",
        "\r\n",
        "try:\r\n",
        "    # Check for existing compute target\r\n",
        "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    # If it doesn't already exist, create it\r\n",
        "    try:\r\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
        "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "        inference_cluster.wait_for_completion(show_output=True)\r\n",
        "    except Exception as ex:\r\n",
        "        print(ex)\r\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192254594
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline will need an environment in which to run, so we'll create a Conda specification that includes the packages that the code uses.\r\n",
        "\r\n",
        "_Note : Use Python > 3.6, Joblib latest version with python 3.6 gives an error while you load a model. _"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\r\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
        "\r\n",
        "# Create an Environment for the experiment\r\n",
        "batch_env = Environment.from_conda_specification(\"experiment_batch_env\", \"environment.yml\")\r\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
        "print('Configuration ready.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192257557
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use a pipeline to run the batch prediction script, generate predictions from the input data, and save the results as a text file in the output folder. To do this, we can use a ParallelRunStep, which enables the batch data to be processed in parallel and the results collated in a single output file named _parallel_run_step.txt._\r\n",
        "\r\n",
        "[OutputFileDatasetConfig](https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.data.outputfiledatasetconfig?view=azure-ml-py)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\r\n",
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "\r\n",
        "output_dir = OutputFileDatasetConfig(name='inferences')\r\n",
        "\r\n",
        "parallel_run_config = ParallelRunConfig(\r\n",
        "    source_directory=experiment_folder,\r\n",
        "    entry_script=\"batch_diabetes.py\",\r\n",
        "    mini_batch_size=\"5\",\r\n",
        "    error_threshold=10,\r\n",
        "    output_action=\"append_row\",\r\n",
        "    environment=batch_env,\r\n",
        "    compute_target=inference_cluster,\r\n",
        "    node_count=2)\r\n",
        "\r\n",
        "parallelrun_step = ParallelRunStep(\r\n",
        "    name='batch-score-diabetes',\r\n",
        "    parallel_run_config=parallel_run_config,\r\n",
        "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\r\n",
        "    output=output_dir,\r\n",
        "    arguments=[],\r\n",
        "    allow_reuse=True\r\n",
        ")\r\n",
        "\r\n",
        "print('Steps defined')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192267636
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Put the step into a pipeline, and run it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\r\n",
        "pipeline_run = Experiment(ws, 'predict-diabetes-pipeline').submit(pipeline)\r\n",
        "RunDetails(pipeline_run).show()\r\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192284324
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the pipeline has finished running, the resulting predictions will have been saved in the outputs of the experiment associated with the first (and only) step in the pipeline. We can retrieve it as follows:\r\n",
        "```\r\n",
        "\r\n",
        "I've tried to retrived the output but found there was a [BUG](https://github.com/Azure/azure-sdk-for-python/issues/23784) and it didn't get resolved while I was working. So I'm writing all the steps here to retrieve the output. Hopefully when you work on this tutorial it will get resolved.\r\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "import shutil\r\n",
        "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\r\n",
        "\r\n",
        "# Remove the local results folder if left over from a previous run\r\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\r\n",
        "\r\n",
        "# Get the run for the first step and download its output\r\n",
        "step_run = next(pipeline_run.get_steps())\r\n",
        "prediction_output = step_run.get_output_data('inferences')\r\n",
        "prediction_output.download(local_path='diabetes-results')\r\n",
        "\r\n",
        "# Traverse the folder hierarchy and find the results file\r\n",
        "for root, dirs, files in os.walk('diabetes-results'):\r\n",
        "    for file in files:\r\n",
        "        if file.endswith('parallel_run_step.txt'):\r\n",
        "            result_file = os.path.join(root,file)\r\n",
        "\r\n",
        "# cleanup output format\r\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\r\n",
        "df.columns = [\"File\", \"Prediction\"]\r\n",
        "\r\n",
        "# Display the first 20 results\r\n",
        "df.head(20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669135329850
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Publish the Pipeline and use its REST Interface\r\n",
        "\r\n",
        "Now that we have a working pipeline for batch inferencing, we can publish it and use a REST endpoint to run it from an application."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\r\n",
        "    name='diabetes-batch-pipeline', description='Batch scoring of diabetes data', version='1.0')\r\n",
        "\r\n",
        "published_pipeline"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192362634
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the published pipeline has an endpoint, which we can see in the Azure portal. We can also find it as a property of the published pipeline object:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "print(rest_endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192371594
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the endpoint, client applications need to make a REST call over HTTP. This request must be authenticated, so an authorization header is required. To test this out, we'll use the authorization header from our current connection to our Azure workspace, which we can get using the following code:\r\n",
        "\r\n",
        "> **Note**: A real application would require a service principal with which to be authenticated."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\r\n",
        "\r\n",
        "interactive_auth = InteractiveLoginAuthentication()\r\n",
        "auth_header = interactive_auth.get_authentication_header()\r\n",
        "print('Authentication header ready.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Authentication header ready.\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192376244
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to call the REST interface. The pipeline runs asynchronously, so we'll get an identifier back, which we can use to track the pipeline experiment as it runs:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\r\n",
        "\r\n",
        "rest_endpoint = published_pipeline.endpoint\r\n",
        "response = requests.post(rest_endpoint, \r\n",
        "                         headers=auth_header, \r\n",
        "                         json={\"ExperimentName\": \"predict-diabetes-pipeline\"})\r\n",
        "run_id = response.json()[\"Id\"]\r\n",
        "run_id"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192379419
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have the run ID, we can use the **RunDetails** widget to view the experiment as it runs:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\r\n",
        "from azureml.widgets import RunDetails\r\n",
        "\r\n",
        "published_pipeline_run = PipelineRun(ws.experiments['predict-diabetes-pipeline'], run_id)\r\n",
        "\r\n",
        "# Block until the run completes\r\n",
        "published_pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669192381726
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait for the pipeline run to complete. As before, the results are in the output of the first pipeline step. Rewrite the code to see the output"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\r\n",
        "\r\n",
        "Here we learn how to create a batch inference pipeline to predict the result. And we learn how to publish the pipeline and use its endpoint for a prediction application."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}