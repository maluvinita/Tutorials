{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Tutorial: Train a diabetes prediction model with Azure Machine Learning and score with ADX\n",
        "\n",
        "[This dataset](https://github.com/maluvinita/Tutorials/tree/main/data) is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n",
        "\n",
        "# Prerequisite\n",
        "- Enable Python plugin on your ADX cluster (see the Onboarding section of the [python() plugin](https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin?pivots=azuredataexplorer) doc)\n",
        "- Whitelist a blob container to be accessible by ADX Python sandbox (see the Appendix section of the doc)\n",
        "- Create a Python environment (conda or virtual env) that reflects the Python sandbox image\n",
        "- Install in that environment AML SDK\n",
        "- Install in that environment Azure Blob Storage SDK (intall the older version v2.1 as the newer version is currently incompatible with azure-kusto-ingest package)\n",
        "\n",
        "# Set up your AML environment\n",
        "- 1. Import Python packages\n",
        "- 2. Create (or connect to) an AML workspace\n",
        "- 3. Create (or connect to) a remote compute target to use for training\n",
        "- 4. Create an experiment to track all your runs\n",
        "\n",
        "## Instructions:\n",
        "- [Kusto Query Language (KQL) overview](https://learn.microsoft.com/en-us/azure/data-explorer/kusto/query/) \n",
        "- When I created this module ADX supported python 3.6.* & azure-storage-blob=2.1 therefore I chose specific version of library but in future you might go for the latest version. As they are going to upgrade python version to 3.9.12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Importing AML packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668094617908
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import Experiment\n",
        "from azureml.core.compute import AmlCompute\n",
        "from azureml.core.compute import ComputeTarget\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "\n",
        "print(sys.version)\n",
        "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create workspace\n",
        "If the workspace already exists connect to it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668094538280
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ws = Workspace.create(\n",
        "    name = \"Your Workspace Name\",\n",
        "    subscription_id = \"Your Subsription Id\",\n",
        "    resource_group = \"Your Resource Group\", \n",
        "    location = \"Your location\",  # e.g \"westus\"\n",
        "    exist_ok = True,\n",
        "    show_output = True)\n",
        "\n",
        "ws.write_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1668082654913
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vmalu-ml\tuksouth\tvmalu-rg\n"
          ]
        }
      ],
      "source": [
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create experiment\n",
        "Create an experiment to track the runs in your workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "exp = Experiment(workspace=ws, name=\"Prediction-Diabetes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create or attach existing compute resource\n",
        "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines.Here you create Azure Machine Learning Compute for model training\n",
        "\n",
        "**Creation of compute takes approximately 5 minutes**. If the AmlCompute with that name is already in your workspace the code will skip the creation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668082659081
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "compute_name = \"vmalu-ci\"\n",
        "vm_sku = \"STANDARD_E4AS_V4\"\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    if compute_target and type(compute_target) is azureml.core.compute.computeinstance.ComputeInstance:\n",
        "        print(\"found compute target: \" + compute_name)\n",
        "else:\n",
        "    print(\"creating new compute target...\")\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_sku, min_nodes=1,max_nodes=2)\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Install Kqlmagic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install Kqlmagic --no-cache-dir  --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Note: KQLMAGIC_AZUREML_COMPUTE environment variable should be set, otherwise popup windows might not function properly.**\n",
        "\n",
        "Web address of your compute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1668082667720
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KQLMAGIC_AZUREML_COMPUTE']=\"https://ml.azure.com/compute/{-}/details?wsid=/subscriptions/{-}/resourcegroups/{-}/providers/Microsoft.MachineLearningServices/workspaces/{-}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load kql \n",
        "%reload_ext Kqlmagic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Connect to the Azure Data Explorer\n",
        "Authenticate yourself! Follow the steps you get in the output   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%kql kusto://code;cluster='your adx cluser ';database='your database'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Explore data\n",
        "Before you train a model, you need to understand the data that you are using to train it.\n",
        "- Fetch the diabetic prediction dataset from Kusto using KqlMagic\n",
        "- Display some records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668082705205
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%kql res << Diabetes # Save dataset in the Diabetes table \n",
        "df = res.to_dataframe() \n",
        "print(df.shape)\n",
        "df[:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Let's copy the data from ADX to blob container to access it from AML\n",
        "Notes:\n",
        "- We copy the input data using KqlMagic to a blob container in the storage account that was allocated for the AML workspace\n",
        "- You can create the blob container using Azure Storage Explorer, and extract its SAS token by right clicking it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668083483219
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "aml_storage_account = \"Your storage account\" # you can use the storage account that was created automatically as part of the AML workspace\n",
        "aml_container_name = \"kusto\"\n",
        "aml_sas_token = \"Your SAS Token for this container\"\n",
        "aml_stroage_key = \"Your storage account key \"\n",
        "os.environ['AZURE_STORAGE_CONNECTION_STRING']= \"Your storage account connection string\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668082714046
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "blob_container_uri = f\"https://{aml_storage_account}.blob.core.windows.net/{aml_container_name};{aml_stroage_key}\"\n",
        "copy_query = f\".export to csv (h@'{blob_container_uri}') with(namePrefix=Diabetes, includeHeaders=all) <| Diabetes\"\n",
        "print(copy_query)\n",
        "\n",
        "%kql res << -query copy_query\n",
        "data_blob_name = res.to_dataframe()[\"Path\"][0].split('/')[-1]\n",
        "print(\"\\ndata blob name is: \", data_blob_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Note:** This notebook supports the latest version of azure-storage-blob, I used the latest API and modules to download/upload  blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668093780417
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Test downloading the blob\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient, ContainerClient, __version__\n",
        "import pandas as pd\n",
        "try:\n",
        "    print(\"Azure Blob Storage v\" + __version__ + \" - Python quickstart sample\")\n",
        "    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
        "\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "    container_client = blob_service_client.get_container_client(aml_container_name)\n",
        "    local_path = './'\n",
        "    download_file_path = os.path.join(local_path, 'Diabetes.csv')\n",
        "    with open(download_file_path, \"wb\") as download_file:\n",
        "        download_file.write(container_client.download_blob(data_blob_name).readall())\n",
        "        print( data_blob_name + \" downloaded\")\n",
        "\n",
        "except Exception as ex:\n",
        "    print('Exception:')\n",
        "    print(ex)\n",
        "\n",
        "df = pd.read_csv('Diabetes.csv')\n",
        "print(df.shape)\n",
        "df[-4:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Training on a remote cluster\n",
        "Here we submit the job to run on the remote training cluster we set up earlier. To submit a job we:\n",
        "- Create a directory for all files to be uploaded to the remote cluster\n",
        "- Create a training script\n",
        "- Create an estimator object\n",
        "- Submit the job\n",
        "## Create a directory\n",
        "Create a directory to upload all files to the remote cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "gather": {
          "logged": 1668093794833
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "script_folder = os.path.join(os.getcwd(), \"to-upload\")\n",
        "os.makedirs(script_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create a training script\n",
        "To submit the job to the cluster, we need to create a training script. Here we create train.py in the to-upload directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%%writefile \"$script_folder/train.py\"\n",
        "\n",
        "# Import libraries\n",
        "from azureml.core import Run\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from azure.storage.blob import BlockBlobService\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--account', type=str,  help='storage account name')\n",
        "parser.add_argument('--container', type=str, help='blob container name')\n",
        "parser.add_argument('--blob', type=str, help='blob name')\n",
        "parser.add_argument('--sas', type=str,  help='SAS token')\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "storage_account = args.account\n",
        "container_name = args.container\n",
        "blob_name = args.blob\n",
        "sas_token = args.sas\n",
        "\n",
        "try:\n",
        "    block_blob_service = BlockBlobService(account_name=storage_account, sas_token=sas_token)\n",
        "    block_blob_service.get_blob_to_path(container_name, blob_name, 'diabetes.csv')\n",
        "\n",
        "except Exception as ex:\n",
        "    print('Exception:')\n",
        "    print(ex)\n",
        "\n",
        "\n",
        "# load the diabetes dataset\n",
        "print(\"Loading Data...\")\n",
        "diabetes = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Set regularization hyperparameter\n",
        "reg = 0.01\n",
        "\n",
        "# Train a logistic regression model\n",
        "print('Training a logistic regression model with regularization rate of', reg)\n",
        "run.log('Regularization Rate',  np.float(reg))\n",
        "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# calculate AUC\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "# Save the trained model in the outputs folder\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "\n",
        "with open('outputs/diabetes_model.pkl', 'wb') as handle:\n",
        "    pickle.dump(model, handle)\n",
        "\n",
        "run.complete()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Run the training script as an experiment\n",
        "Now you're ready to run the script as an experiment. Note that the default environment does not include the ML/azure packages, so you need to explicitly add that to the configuration. The conda environment is built on-demand the first time the experiment is run, and cached for future runs that use the same configuration; so the first run will take a little longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668094546690
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create Environment to install required packages\n",
        "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
        "from azureml.core.runconfig import DockerConfiguration\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "env = Environment.from_conda_specification(\"adx_sandbox_env\", \"environment.yml\")\n",
        "\n",
        "\n",
        "script_config = ScriptRunConfig(source_directory=script_folder,\n",
        "              arguments=['--account', aml_storage_account, '--container',aml_container_name, '--blob', data_blob_name, '--sas', aml_sas_token],\n",
        "              environment=env,\n",
        "              script='train.py')\n",
        "\n",
        "from azureml.widgets import RunDetails\n",
        "run = exp.submit(config=script_config)\n",
        "RunDetails(run).show()\n",
        "# specify show_output to True for a verbose log\n",
        "run.wait_for_completion(show_output=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Register model\n",
        "The training script pickled the models to files and wrote them in a directory named outputs in the VM of the cluster where the job is executed. outputs is a special directory in that all content in this directory is automatically uploaded to our workspace. This content appears in the run record in the experiment under the workspace. Hence, the model file is now also available in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668089023166
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model = run.register_model(model_name='Diabetic-Prediction', model_path='outputs/diabetes_model.pkl')\n",
        "print(model.name, model.id, model.version, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Scoring in ADX\n",
        "2 options for retrieving the model for scoring:\n",
        "\n",
        "- serialize the model to a string to be stored in a standard table in ADX   (It is applicable for a small size of model otherwise you will get this error \"_Request is invalid and cannot be processed: Syntax error: SYN0009: Query length (354646170) too large (max: 2097152) [line:position=0:0]_\")\n",
        "- copy the model to a blob container (that was previously whitelisted for access by ADX Python sandbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668089286455
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "model_path = model.download(exist_ok=True)\n",
        "model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Copy the model to a blob container "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668088583206
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
        "try:\n",
        "    print(\"Azure Blob Storage v\" + __version__ + \" - Python quickstart sample\")\n",
        "    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
        "\n",
        "    model_name = 'diabetes_model.pkl'\n",
        "    upload_file_path = os.path.join('./', model_path)\n",
        "    \n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "    blob_client = blob_service_client.get_blob_client(container=aml_container_name, blob=model_name)\n",
        "\n",
        "    print(\"\\nUploading to Azure Storage as blob:\\n\\t\" + model_name)\n",
        "\n",
        "    # Upload the created file\n",
        "    with open(upload_file_path, \"rb\") as data:\n",
        "        blob_client.upload_blob(data)\n",
        "    \n",
        "except Exception as ex:\n",
        "    print('Exception:')\n",
        "    print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668092058811
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Copy Blob SAS URL of the uploaded model \n",
        "model_uri = \" Blob SAS URL \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Scoring from model which is stored in blob storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668092504988
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "scoring_from_blob_query = r'''\n",
        "let classify_sf=(samples:(*), model_sas:string, features_cols:dynamic, pred_col:string)\n",
        "{\n",
        "    let kwargs = pack('model_sas', model_sas, 'features_cols', features_cols, 'pred_col', pred_col);\n",
        "    let code =\n",
        "    '\\n'\n",
        "    'import pickle\\n'\n",
        "    '\\n'\n",
        "    'model_sas = kargs[\"model_sas\"]\\n'\n",
        "    'features_cols = kargs[\"features_cols\"]\\n'\n",
        "    'pred_col = kargs[\"pred_col\"]\\n'\n",
        "    'with open(\"/Temp/model.pkl\", \"rb\") as f:\\n'\n",
        "    '   bmodel = f.read()\\n'\n",
        "    'clf1 = pickle.loads(bmodel)\\n'\n",
        "    'df1 = df[features_cols]\\n'\n",
        "    'predictions = clf1.predict(df1)\\n'\n",
        "    '\\n'\n",
        "    'result = df\\n'\n",
        "    'result[pred_col] = pd.DataFrame(predictions, columns=[pred_col])'\n",
        "    '\\n'\n",
        "    ;\n",
        "    samples | evaluate python(typeof(*), code, kwargs,\n",
        "        external_artifacts=pack('model.pkl', model_sas))\n",
        "};\n",
        "Diabetes \n",
        "| extend pred_Diabetic=0\n",
        "| invoke classify_sf('$model_uri$',\n",
        "                     pack_array('Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age'), 'pred_Diabetic')\n",
        "| summarize n=count() by Diabetic, pred_Diabetic      //  confusion matrix\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668092508018
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "scoring_from_blob_query = scoring_from_blob_query.replace('$model_uri$', model_uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1668092543053
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%kql res << -query scoring_from_blob_query\n",
        "df = res.to_dataframe()\n",
        "print('Confusion Matrix')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Summary\n",
        "In this tutorial you learned how to train a model in AML and then use ADX for scoring. \n",
        "- ADX scoring is done near the data, on the same ADX compute nodes, enabling near real time processing of big amounts of new data. There is no the need to export the data to external scoring service and import back the results. Consequently, scoring architecture is simpler and performance is much faster and scalable"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
